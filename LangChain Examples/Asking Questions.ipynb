{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e7d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f1b05",
   "metadata": {},
   "source": [
    "## Load document from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2648a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../LangChainDocs/Findings.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8c50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef20a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Study\\n1Study\\nOriginal Documentation\\n https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\\nMy Findings\\nPrompt Engineering, also known as In-Context Prompting, is a method used to guide  \\nthe behavior of autoregressive language models (LLMs) without modifying their weights.  \\nThe goal is to align the model's output with the desired outcomes by carefully designing  \\nprompts. This approach requires experimentation and heuristic methods due to the  \\nvariation in the ef fectiveness of prompt engineering across dif ferent models.\\nZero-shot learning and few-shot learning are two basic approaches used for prompting  \\nLLMs. In zero-shot learning, the task text is directly fed to the model to generate results.  \\nFew-shot learning, on the other hand, involves providing a set of high-quality  \\ndemonstrations that include input and desired output examples. Few-shot learning  \\ngenerally leads to better performance than zero-shot learning, but it consumes more  \\ntokens and may be limited by the context length.\\nThe selection of in-context examples plays a crucial role in prompt engineering. Studies  \\nhave shown that the choice of prompt format, training examples, and their order can  \\nsignificantly af fect the model's performance. T echniques such as -NN clustering in the  \\nembedding space, graph-based approaches, and Q-Learning have been explored to  \\nselect semantically similar and diverse examples.\\nExample ordering is another important aspect of prompt engineering. It is recommended  \\nto keep the selection of examples diverse, relevant to the test sample, and in a random  \\norder to avoid biases like majority label bias and recency bias. The order that works well  \\nfor one model may not work for another , so careful consideration is required.\\nInstruction prompting is another approach used in prompt engineering, where high-\\nquality tuples of task instructions, input, and ground truth output are used to finetune  \\npretrained models. Reinforcement Learning from Human Feedback (RLHF) is a  \\ncommon method used for instruction following-style fine-tuning. It improves the model's  \\nalignment with human intention and reduces communication costs.\", metadata={'source': '../LangChainDocs/Findings.pdf', 'page': 0}),\n",
       " Document(page_content='Study\\n2Self-consistency sampling is a technique where multiple outputs are sampled with  \\ntemperature and the best candidate is selected based on specific criteria. Chain-of-\\nThought (CoT) prompting is another method that generates reasoning chains or  \\nrationales step by step to guide the model towards the final answer . CoT prompting can  \\nbe categorized as few-shot CoT or zero-shot CoT , depending on whether  \\ndemonstrations are provided or natural language statements are used to encourage  \\nreasoning.\\nVarious tips and extensions have been proposed to improve prompt engineering, such  \\nas using ensemble learning, incorporating explanations in prompts, and combining CoT  \\nprompting with external search queries. Automatic prompt design is another approach  \\nthat treats prompts as trainable parameters and optimizes them directly on the  \\nembedding space using techniques like AutoPrompt, Prefix-T uning, and Prompt-T uning.\\nOverall, prompt engineering is an empirical science that requires careful  \\nexperimentation and exploration of dif ferent techniques to ef fectively steer the behavior  \\nof autoregressive language models without modifying their weights.\\nPrompt engineering is a technique for communicating with large language models  \\n(LLMs) to steer their behavior for desired outcomes without updating the model weights.  \\nIt is an empirical science and the ef fect of prompt engineering methods can vary a lot  \\namong models, thus requiring heavy experimentation and heuristics.\\nThe article discusses several dif ferent prompt engineering techniques, including:\\nExplicitly providing the desired output. \\xa0This is the simplest form of prompt  \\nengineering, and it involves providing the LLM with the desired output as a prompt.  \\nFor example, if you want the LLM to generate a poem, you could provide the prompt  \\n\"Write me a poem about love.\"\\nUsing chain-of-thought prompts. \\xa0Chain-of-thought prompts are a more advanced  \\nform of prompt engineering that involve providing the LLM with a sequence of  \\nprompts that guide its output. For example, if you want the LLM to write a story , you  \\ncould provide a chain-of-thought prompt that starts with \"Once upon a time,\" then  \\n\"There was a princess named...\", and so on.\\nUsing adversarial prompts. \\xa0Adversarial prompts are a type of prompt engineering  \\nthat involves providing the LLM with two prompts that are contradictory . The LLM is  ', metadata={'source': '../LangChainDocs/Findings.pdf', 'page': 1}),\n",
       " Document(page_content='Study\\n3then forced to choose between the two prompts, which can help to improve the  \\nquality of its output.\\nThe article also discusses some of the challenges of prompt engineering, such as the  \\nfact that it can be dif ficult to find the right prompts for a particular task. However , the \\nauthor argues that prompt engineering is a powerful technique that can be used to  \\nachieve a wide variety of results.', metadata={'source': '../LangChainDocs/Findings.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c37da9",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "638c9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0934fd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab4c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b276ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 5 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now we have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b7a8d",
   "metadata": {},
   "source": [
    "## Creating embeddings and storing in pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60590ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cohlem/anaconda3/envs/learnings/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de49fe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter you openai keys········\n",
      "Enter you pinecone api key········\n",
      "Enter you pinecone env name········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass('Enter you openai keys')\n",
    "\n",
    "PINECONE_API_KEY = getpass('Enter you pinecone api key')\n",
    "PINECONE_API_ENV = getpass('Enter you pinecone env name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "837bbdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"langchaintest\" # put in the name of your pinecone index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29f82c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-west4-gcp-free'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84275bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ec8491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ed0bded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.pinecone.Pinecone at 0x146343510>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffee552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(\"few-shot learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bf8deff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pretrained models. Reinforcement Learning from Human Feedback (RLHF) is a  \\ncommon method used for instruction following-style fine-tuning. It improves the model's  \\nalignment with human intention and reduces communication costs.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133b5f8",
   "metadata": {},
   "source": [
    "## Query with document to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cac13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "483b9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdc8ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88c2e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm, chain_type = \"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c28739f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In-Context Prompting, also known as Prompt Engineering, is a method used to guide the behavior of autoregressive language models (LLMs) without modifying their weights.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents = docs, question = \"what is in context learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa1bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnings",
   "language": "python",
   "name": "learnings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
